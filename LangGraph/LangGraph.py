"""
LangGraph.py RAG íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ (LangGraph ê¸°ë°˜)
====================================================

LangGraph.ipynb ë…¸íŠ¸ë¶ì˜ ì „ì²´ ê¸°ëŠ¥ì„ import ê°€ëŠ¥í•œ Python ëª¨ë“ˆë¡œ ì •ë¦¬.
stream.py ì—ì„œ import í•˜ì—¬ Streamlit ë°ëª¨ì— í™œìš©í•©ë‹ˆë‹¤.

ì‹¤í–‰ ìˆœì„œ (ì˜ì¡´ì„± ê³ ë ¤):
  0.  í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ Â· sys.path ì„¤ì •
  1.  í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env)
  2.  ë¡œê·¸ ìˆ˜ì§‘ê¸° (stream.py â†’ toast ì—°ë™)
  3.  ìƒìˆ˜ ì •ì˜
  4.  ì™¸ë¶€ íŒ¨í‚¤ì§€ import (LangChain, LangGraph ë“±)
  5.  ë¡œì»¬ ëª¨ë“ˆ import (rag íŒ¨í‚¤ì§€)
  6.  GraphState ì •ì˜
  7.  ëª¨ë“ˆ ë ˆë²¨ ë³€ìˆ˜ (ì´ˆê¸°í™” ì‹œ ì„¤ì •)
  8.  ì´ˆê¸°í™” í•¨ìˆ˜
  9.  ë¬¸ì„œ ì ì¬ API
  10. í—¬í¼ í•¨ìˆ˜
  11. ë…¸ë“œ í•¨ìˆ˜
  12. ë¼ìš°íŒ… í•¨ìˆ˜ (conditional_edges ìš©)
  13. ê·¸ë˜í”„ êµ¬ì„± Â· ì»´íŒŒì¼
  14. ê³µê°œ API (query ë“±)
"""

# ============================================================
# 0. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ Â· ê²½ë¡œ ì„¤ì •
# ============================================================
import os
import sys
import json
import re
import uuid
from pathlib import Path
from datetime import datetime, timezone
from typing import TypedDict, Annotated, List, Dict, Any

# ì´ íŒŒì¼ì€ <project_root>/LangGraph/ ì— ìœ„ì¹˜.
# rag íŒ¨í‚¤ì§€ë¥¼ import í•˜ë ¤ë©´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ê°€ sys.path ì— ìˆì–´ì•¼ í•œë‹¤.
_PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(_PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(_PROJECT_ROOT))

# ============================================================
# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# ============================================================
from dotenv import load_dotenv

load_dotenv()  # .env íŒŒì¼ì—ì„œ HF_API_KEY, TAVILY_API_KEY ë“± ë¡œë“œ

# ============================================================
# 2. ë¡œê·¸ ìˆ˜ì§‘ê¸°
#    - ë…¸ë“œ ë‚´ë¶€ print ëŒ€ì‹  _log() ì‚¬ìš©
#    - stream.py ì—ì„œ get_and_clear_logs() â†’ st.toast()
# ============================================================
_log_buffer: List[str] = []


def _log(msg: str):
    """ë‚´ë¶€ ë©”ì‹œì§€ë¥¼ ë²„í¼ì— ì €ì¥í•˜ê³  ì½˜ì†”ì—ë„ ì¶œë ¥"""
    _log_buffer.append(msg)
    print(msg)


def get_and_clear_logs() -> List[str]:
    """ìŒ“ì¸ ë¡œê·¸ë¥¼ ë°˜í™˜í•˜ê³  ë²„í¼ë¥¼ ë¹„ìš´ë‹¤ (stream.py ê°€ í˜¸ì¶œ)."""
    msgs = _log_buffer.copy()
    _log_buffer.clear()
    return msgs


# ============================================================
# 3. ìƒìˆ˜
# ============================================================
PERSIST_DIR = "./chroma_db"  # ChromaDB ì €ì¥ ê²½ë¡œ (LangGraph/ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ)
COLLECTION_MAIN = "my_collection"  # ì£¼ìš” ë¬¸ì„œ ì»¬ë ‰ì…˜
COLLECTION_CHAT_RAW = "chat_history_raw"  # ëŒ€í™” ì›ë³¸ ì €ì¥
COLLECTION_CHAT_SUMMARY = "chat_history_summarized"  # ëŒ€í™” ìš”ì•½ ì €ì¥

# LLM ëª¨ë¸ ì‹ë³„ì
ROUTER_MODEL = "meta-llama/Llama-3.1-8B-Instruct"  # ë¼ìš°íŒ…Â·íŒë‹¨Â·ìš”ì•½ìš©
CHAIN_MODEL = "meta-llama/Meta-Llama-3-8B-Instruct"  # ë‹µë³€ ìƒì„±ìš© (rag.base)
EMBEDDING_MODEL = "BAAI/bge-m3"  # ì„ë² ë”© ëª¨ë¸

MAX_CHARS_PER_DOC = 1500  # ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ì„ê³„ì¹˜ (â‰ˆ1000 í† í°)

# ============================================================
# 4. ì™¸ë¶€ íŒ¨í‚¤ì§€ import
# ============================================================
from langchain_core.documents import Document
from langchain_core.runnables import RunnableConfig
from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint
from langchain_chroma import Chroma
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.graph import END, StateGraph
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

# ============================================================
# 5. ë¡œì»¬ ëª¨ë“ˆ import (rag íŒ¨í‚¤ì§€)
# ============================================================
from rag.base import create_embedding_auto
from rag.chroma import ChromaRetrievalChain
from rag.ingest import ingest_documents as _raw_ingest_docs
from rag.ingest import ingest_pdfs as _raw_ingest_pdfs
from rag.utils import format_docs
from rag.graph_utils import random_uuid  # ì„¸ì…˜ ID ìƒì„±ìš© (re-export)

# ============================================================
# 6. GraphState ì •ì˜
# ============================================================


class GraphState(TypedDict):
    """LangGraph ë…¸ë“œ ê°„ ì „ë‹¬ë˜ëŠ” ìƒíƒœ ë”•ì…”ë„ˆë¦¬"""

    question: Annotated[str, "ì‚¬ìš©ì ì§ˆë¬¸ (ì¬ì‘ì„± í›„ ê°±ì‹ ë¨)"]
    context: Annotated[str, "ê²€ìƒ‰Â·ì›¹ ê²°ê³¼ë¥¼ í•©ì¹œ ë¬¸ë§¥ í…ìŠ¤íŠ¸"]
    answer: Annotated[str, "LLM ì´ ìƒì„±í•œ ìµœì¢… ë‹µë³€"]
    messages: Annotated[list, add_messages]  # ëŒ€í™” ì´ë ¥ (ëˆ„ì )
    relevance: Annotated[str, "ê²€ìƒ‰ ë¬¸ì„œ ê´€ë ¨ì„± yes/no"]


# ============================================================
# 7. ëª¨ë“ˆ ë ˆë²¨ ë³€ìˆ˜ â€” initialize() ì—ì„œ ì„¤ì •ë¨
# ============================================================
_retriever = None  # ChromaDB ê¸°ë°˜ retriever
_chain = None  # RAG ë‹µë³€ ì²´ì¸
_chat_hf = None  # ë¼ìš°íŒ…Â·íŒë‹¨Â·ìš”ì•½ìš© LLM
_embeddings = None  # ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
_app = None  # ì»´íŒŒì¼ëœ LangGraph ì•±
_initialized = False  # ì´ˆê¸°í™” ì™„ë£Œ í”Œë˜ê·¸

# ============================================================
# 8. ì´ˆê¸°í™” í•¨ìˆ˜
# ============================================================


def _init_hf_login():
    """HuggingFace Hub í† í° ë¡œê·¸ì¸"""
    from huggingface_hub import login

    token = os.getenv("HF_API_KEY")
    if token:
        os.environ["HF_API_KEY"] = token
        login(token=token)
        _log("âœ… HuggingFace ë¡œê·¸ì¸ ì„±ê³µ")
    else:
        _log("âš ï¸ HF_API_KEY ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")


def _init_chat_model():
    """ë¼ìš°íŒ… Â· íŒë‹¨ Â· ìš”ì•½ìš© LLM ì´ˆê¸°í™” (Llama-3.1-8B-Instruct)"""
    global _chat_hf
    llm = HuggingFaceEndpoint(
        repo_id=ROUTER_MODEL,
        task="text-generation",
        temperature=0.0,
        max_new_tokens=512,
    )
    _chat_hf = ChatHuggingFace(llm=llm)
    _log(f"âœ… ë¼ìš°íŒ… LLM ë¡œë“œ ì™„ë£Œ: {ROUTER_MODEL}")


def _init_embeddings():
    """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (create_embedding_auto â†’ ë¡œì»¬/API ìë™ ì„ íƒ)"""
    global _embeddings
    _embeddings = create_embedding_auto()
    _log(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {EMBEDDING_MODEL}")


def _init_rag_chain(
    persist_directory: str = PERSIST_DIR,
    collection_name: str = COLLECTION_MAIN,
    k: int = 10,
):
    """ChromaDB ê¸°ë°˜ RAG ì²´ì¸ (retriever + chain) ì´ˆê¸°í™”"""
    global _retriever, _chain
    _log("ğŸš€ ChromaDB ê¸°ë°˜ RAG ì²´ì¸ ìƒì„± ì‹œì‘...")
    rag = ChromaRetrievalChain(
        persist_directory=persist_directory,
        collection_name=collection_name,
        k=k,
    ).create_chain()
    _retriever = rag.retriever
    _chain = rag.chain
    _log("âœ… RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")


def initialize(
    persist_directory: str = PERSIST_DIR,
    collection_name: str = COLLECTION_MAIN,
    k: int = 10,
):
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” â€” ìµœì´ˆ 1 íšŒë§Œ ì‹¤í–‰.

    ìˆœì„œ: HF ë¡œê·¸ì¸ â†’ ë¼ìš°íŒ… LLM â†’ ì„ë² ë”© â†’ RAG ì²´ì¸
    """
    global _initialized
    if _initialized:
        return
    _init_hf_login()
    _init_chat_model()
    _init_embeddings()
    _init_rag_chain(persist_directory, collection_name, k)
    _initialized = True
    _log("âœ… íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")


# ============================================================
# 9. ë¬¸ì„œ ì ì¬ API
# ============================================================


def ingest_uploaded_file(
    file_path: str,
    persist_directory: str = PERSIST_DIR,
    collection_name: str = COLLECTION_MAIN,
):
    """
    ì—…ë¡œë“œëœ ë‹¨ì¼ íŒŒì¼ (PDF / TXT) ì„ ChromaDB ì— ì ì¬.
    stream.py íŒŒì¼ ì—…ë¡œë“œì—ì„œ í˜¸ì¶œ.
    """
    ext = Path(file_path).suffix.lower()
    if ext == ".pdf":
        _raw_ingest_pdfs(
            pdf_paths=[file_path],
            persist_directory=persist_directory,
            collection_name=collection_name,
        )
    else:
        _raw_ingest_docs(
            file_paths=[file_path],
            persist_directory=persist_directory,
            collection_name=collection_name,
        )
    _log(f"âœ… íŒŒì¼ ì ì¬ ì™„ë£Œ: {Path(file_path).name}")


# ============================================================
# 10. í—¬í¼ í•¨ìˆ˜
# ============================================================


def _to_text(msg) -> str:
    """
    ë‹¤ì–‘í•œ ë©”ì‹œì§€ íƒ€ì…ì„ 'role: content' ë¬¸ìì—´ë¡œ ë³€í™˜.
    - LangChain BaseMessage (type, content ì†ì„±)
    - (role, content) íŠœí”Œ/ë¦¬ìŠ¤íŠ¸
    - ê¸°íƒ€ â†’ str()
    """
    if hasattr(msg, "type") and hasattr(msg, "content"):
        return f"{msg.type}: {msg.content}"
    if isinstance(msg, (tuple, list)) and len(msg) >= 2:
        return f"{msg[0]}: {msg[1]}"
    return str(msg)


def _extract_question(raw) -> str:
    """state['question'] ì´ ì–´ë–¤ íƒ€ì…ì´ë“  ìˆœìˆ˜ ë¬¸ìì—´ë¡œ ì¶”ì¶œ"""
    if hasattr(raw, "content"):
        return str(raw.content)
    if isinstance(raw, (list, tuple)) and raw:
        last = raw[-1]
        return str(last.content) if hasattr(last, "content") else str(last)
    return str(raw)


def _looks_ambiguous(q: str) -> bool:
    """ì§§ê±°ë‚˜ ëŒ€ëª…ì‚¬ Â· ëª¨í˜¸ í‘œí˜„ì´ í¬í•¨ëœ ì§ˆë¬¸ì¸ì§€ íœ´ë¦¬ìŠ¤í‹± íŒë³„"""
    q = (q or "").strip()
    if not q:
        return False
    ambiguous = [
        "ê·¸ê±°", "ê·¸ê²ƒ", "ì´ê±°", "ì €ê±°", "ê·¸ë•Œ", "ì €ë²ˆ", "ì•„ê¹Œ",
        "ê·¸ ë‚´ìš©", "ê·¸ ì´ì•¼ê¸°", "ê¸°ì–µë‚˜", "ê¸°ì–µí•´", "ë‹¤ì‹œ", "ì´ì–´",
        "ë” ìì„¸íˆ", "ë­ì˜€ì§€",
    ]
    short_followups = ["ì™œ?", "ì–´ì§¸ì„œ?", "ë­ì•¼?", "ë­”ë°?", "ê·¸ê²Œ ë­ì•¼?", "ì„¤ëª…í•´ì¤˜"]
    return any(t in q for t in ambiguous) or q in short_followups or len(q) <= 8


def _message_to_role_content(msg):
    """ë©”ì‹œì§€ â†’ (role, content) íŠœí”Œ ë³€í™˜"""
    if hasattr(msg, "type") and hasattr(msg, "content"):
        role = {"human": "user", "ai": "assistant"}.get(msg.type, msg.type)
        return role, str(msg.content)
    if isinstance(msg, (tuple, list)) and len(msg) >= 2:
        return str(msg[0]), str(msg[1])
    return "unknown", str(msg)


def _conversation_only(messages) -> list:
    """user/assistant ì—­í• ì˜ ë©”ì‹œì§€ë§Œ í•„í„°ë§"""
    conv = []
    for m in messages:
        role, content = _message_to_role_content(m)
        if role in {"user", "assistant", "human", "ai"}:
            conv.append((role, content))
    return conv


def _summarize_if_long(content: str, max_chars: int = MAX_CHARS_PER_DOC) -> str:
    """í…ìŠ¤íŠ¸ê°€ max_chars ë¥¼ ì´ˆê³¼í•˜ë©´ _chat_hf ë¡œ ìš”ì•½"""
    if len(content) <= max_chars:
        return content
    prompt = (
        f"ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ í•µì‹¬ë§Œ ë‚¨ê²¨ {max_chars}ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. "
        f"í•œê¸€ë¡œ ì‘ì„±í•˜ê³  ë¶ˆí•„ìš”í•œ ë°˜ë³µì€ ì œê±°í•˜ì„¸ìš”. ìš”ì•½ë§Œ ì¶œë ¥.\n\n"
        f"---\n{content[:8000]}\n---"
    )
    try:
        resp = _chat_hf.invoke(prompt)
        text = (resp.content if hasattr(resp, "content") else str(resp)).strip()
        return text[:max_chars]
    except Exception:
        return content[:max_chars] + "..."


# ============================================================
# 11. ë…¸ë“œ í•¨ìˆ˜
# ============================================================


def contextualize(state: GraphState) -> GraphState:
    """
    [contextualize ë…¸ë“œ]
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê³¼ê±° ëŒ€í™” ë§¥ë½ì´ í•„ìš”í•œì§€ íŒë‹¨.
    í•„ìš” ì‹œ chat_history_summarized ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰ í›„ ì§ˆë¬¸ì„ ì¬ì‘ì„±.

    íŒë‹¨ ê¸°ì¤€ (OR ì¡°ê±´):
      1) í‚¤ì›Œë“œ ë§¤ì¹­ (ê·¸ë•Œ, ì €ë²ˆì—, ì•„ê¹Œ, â€¦)
      2) ëª¨í˜¸í•œ í‘œí˜„ ê°ì§€ (_looks_ambiguous)
      3) LLM íŒë‹¨ (recall_judgment_prompt)
    """
    messages = state.get("messages", [])
    question = _extract_question(state.get("question", "")).strip()
    # ìµœê·¼ ëŒ€í™” 10 ë©”ì‹œì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    recent_chat = "\n".join(_to_text(m) for m in messages[-10:])

    # â”€â”€ 1) recall í•„ìš” ì—¬ë¶€ íŒë‹¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    keyword_recall = any(
        kw in question
        for kw in [
            "ê·¸ë•Œ", "ì €ë²ˆì—", "ì•„ê¹Œ", "ì´ì „", "ê¸°ì–µë‚˜",
            "ìœ„ì—", "ê·¸ê±°", "ë‚´ ìƒì¼", "ë‚´ ì •ë³´",
        ]
    )
    ambiguous_recall = _looks_ambiguous(question)

    llm_recall = False
    judge_prompt = f"""ë‹¹ì‹ ì€ ì§ˆì˜ ë¼ìš°íŒ… íŒë³„ê¸°ì…ë‹ˆë‹¤.
ì•„ë˜ ì‚¬ìš©ì ì§ˆë¬¸ì´ ê³¼ê±° ëŒ€í™” ë§¥ë½(íŠ¹íˆ ê°œì¸ ì •ë³´/ì´ì „ ëŒ€í™” ìš”ì•½) ì—†ì´ëŠ” í•´ì„ì´ ì–´ë ¤ìš´ì§€ íŒë‹¨í•˜ì„¸ìš”.

[Recent Chat]
{recent_chat}

[Question]
{question}

ì¶œë ¥ì€ ë°˜ë“œì‹œ ì•„ë˜ ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ:
YES
NO""".strip()

    try:
        resp = _chat_hf.invoke(judge_prompt)
        text = (resp.content if hasattr(resp, "content") else str(resp)).strip().upper()
        llm_recall = "YES" in text
    except Exception:
        pass

    is_recall_needed = keyword_recall or ambiguous_recall or llm_recall
    rewrite_question = question
    long_term_context = ""

    # â”€â”€ 2) recall í•„ìš” ì‹œ â†’ ìš”ì•½ DB ê²€ìƒ‰ â†’ ì§ˆë¬¸ ì¬ì‘ì„± â”€â”€â”€â”€â”€â”€
    if is_recall_needed:
        _log("ğŸ” ê³¼ê±° ëŒ€í™” ìš”ì•½ DB ê²€ìƒ‰ ì¤‘...")

        summary_store = Chroma(
            persist_directory=PERSIST_DIR,
            collection_name=COLLECTION_CHAT_SUMMARY,
            embedding_function=_embeddings,
        )

        # ê²€ìƒ‰ ì¹œí™”ì  ì¿¼ë¦¬ ìƒì„±
        rq_prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸ìœ¼ë¡œ ë²¡í„° ê²€ìƒ‰í•  ì¿¼ë¦¬ë¥¼ 1 ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
- ê³¼ê±° ëŒ€í™”ì—ì„œ ì°¾ì•„ì•¼ í•  í•µì‹¬ ì—”í‹°í‹°ë¥¼ í¬í•¨í•˜ì„¸ìš”.
- ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ ì—†ì´ ê²€ìƒ‰ ì¹œí™”ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- ì§ˆë¬¸ì— ë‹µí•˜ì§€ ë§ê³  ê²€ìƒ‰ ì¿¼ë¦¬ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

[Recent Chat]
{recent_chat}

[Question]
{question}""".strip()

        retrieval_query = question
        try:
            rq = _chat_hf.invoke(rq_prompt)
            cand = (rq.content if hasattr(rq, "content") else str(rq)).strip()
            if cand:
                retrieval_query = cand
        except Exception:
            pass

        docs = summary_store.similarity_search(retrieval_query, k=3)
        if docs:
            long_term_context = "\n".join(d.page_content for d in docs)

        # ì§ˆë¬¸ ì¬ì‘ì„±
        rewrite_prompt = f"""You are a query rewriter.
Rewrite the user's question to be clear and standalone.
Use retrieved long-term context if available. If not available, use only recent chat.
Do not answer. Return only one rewritten question in Korean.

[Recent Chat]
{recent_chat}

[Retrieved Long-term Context]
{long_term_context}

[Original Question]
{question}""".strip()

        try:
            resp = _chat_hf.invoke(rewrite_prompt)
            cand = (resp.content if hasattr(resp, "content") else str(resp)).strip()
            if cand:
                rewrite_question = cand
        except Exception:
            rewrite_question = question

    return GraphState(question=rewrite_question)


def retrieve(state: GraphState) -> GraphState:
    """
    [retrieve ë…¸ë“œ]
    ChromaDB retriever ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰.
    """
    docs = _retriever.invoke(state["question"])
    return GraphState(context=format_docs(docs))


def llm_answer(state: GraphState) -> GraphState:
    """
    [llm_answer ë…¸ë“œ]
    RAG ì²´ì¸ì„ í˜¸ì¶œí•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±.
    ë‹µë³€ê³¼ í•¨ê»˜ (user, assistant) ë©”ì‹œì§€ ìŒì„ messages ì— ì¶”ê°€.
    """
    question = state["question"]
    context = state.get("context", "")
    chat_history = state.get("messages", [])

    try:
        response = _chain.invoke(
            {
                "question": question,
                "context": context,
                "chat_history": chat_history,
            }
        )
    except Exception as e:
        _log(f"âŒ LLM ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {type(e).__name__}: {e}")
        raise

    return GraphState(
        answer=response,
        messages=[("user", question), ("assistant", response)],
    )


def relevance_check(state: GraphState) -> GraphState:
    """
    [relevance_check ë…¸ë“œ]
    ê²€ìƒ‰ëœ ë¬¸ì„œ(context)ê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ”ì§€ _chat_hf ë¡œ í‰ê°€.
    ê²°ê³¼ë¥¼ state['relevance'] = 'yes' | 'no' ë¡œ ì €ì¥.
    """
    prompt = f"""You are a grader assessing whether a retrieved document is relevant to the given question.
Return ONLY valid JSON like: {{"score": "yes"}} or {{"score": "no"}}.

Question:
{state["question"]}

Retrieved document:
{state["context"]}""".strip()

    resp = _chat_hf.invoke(prompt)
    text = resp.content.strip()

    # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ëª¨ë¸ì´ ì•ë’¤ì— í…ìŠ¤íŠ¸ë¥¼ ì„ëŠ” ê²½ìš° ëŒ€ë¹„)
    match = re.search(r"\{.*\}", text, re.DOTALL)
    if match:
        text = match.group(0)

    try:
        data = json.loads(text)
        score = data.get("score", "no").lower()
    except Exception:
        score = "no"

    if score not in ("yes", "no"):
        score = "no"

    _log(f"ğŸ“‹ ê´€ë ¨ì„± í‰ê°€: {score}")
    return {"relevance": score}


def web_search(state: GraphState) -> GraphState:
    """
    [web_search ë…¸ë“œ]
    Tavily API ë¡œ ì›¹ ê²€ìƒ‰ í›„ ê²°ê³¼ë¥¼ context ì— ì €ì¥.
    ê²€ìƒ‰ ê²°ê³¼ëŠ” ChromaDB(my_collection)ì—ë„ ì ì¬í•˜ì—¬ ì¬í™œìš©.
    """
    _log("ğŸŒ ì›¹ ê²€ìƒ‰ ì‹œì‘...")
    tavily = TavilySearchResults(max_results=5, search_depth="basic")
    query_text = state["question"]
    results = tavily.invoke(query_text)

    # ê²°ê³¼ í¬ë§·íŒ… (ê¸´ ë³¸ë¬¸ì€ ìš”ì•½)
    parts = []
    for r in results:
        url = r.get("url", "")
        content = _summarize_if_long(r.get("content", ""))
        parts.append(f"{content}\nì¶œì²˜: {url}")
    formatted = "\n\n---\n\n".join(parts)

    # ChromaDB ì—ë„ ì €ì¥
    if formatted.strip():
        doc = Document(
            page_content=formatted,
            metadata={
                "source": f"web_search:{query_text}",
                "origin": "tavily_merged",
            },
        )
        try:
            _raw_ingest_docs(
                documents=[doc],
                persist_directory=PERSIST_DIR,
                collection_name=COLLECTION_MAIN,
            )
            _log("âœ… ì›¹ ê²€ìƒ‰ ê²°ê³¼ ChromaDB ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            _log(f"âš ï¸ ì›¹ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    return GraphState(context=formatted)


def save_memory(state: GraphState) -> GraphState:
    """
    [save_memory ë…¸ë“œ]
    ëˆ„ì  ëŒ€í™”ê°€ ì¶©ë¶„í•  ë•Œ ì˜¤ë˜ëœ 5 í„´(10 messages)ì„
    raw / summary ì»¬ë ‰ì…˜ì— ê°ê° ì €ì¥.
    """
    messages = state.get("messages", [])
    conv = _conversation_only(messages)
    MIN_MSGS = 10  # 5 í„´ = 10 ë©”ì‹œì§€

    if len(conv) < MIN_MSGS:
        _log(f"â„¹ï¸ save_memory ê±´ë„ˆëœ€: ëŒ€í™” {len(conv)}ê°œ (< {MIN_MSGS})")
        return {}

    oldest = conv[:MIN_MSGS]
    raw_text = "\n".join(f"{r}: {c}" for r, c in oldest).strip()
    if not raw_text:
        return {}

    # â”€â”€ ìš”ì•½ ìƒì„± â”€â”€
    summary_prompt = f"""ë‹¤ìŒì€ ì‚¬ìš©ì-ì–´ì‹œìŠ¤í„´íŠ¸ ëŒ€í™”ì˜ ì˜¤ë˜ëœ 5 í„´ì…ë‹ˆë‹¤.
í•µì‹¬ ì‚¬ì‹¤(ê°œì¸ì •ë³´/ì„ í˜¸/ì•½ì†/ì¤‘ìš” ë§¥ë½)ë§Œ í•œêµ­ì–´ë¡œ 4~6 ë¬¸ì¥ ë‚´ë¡œ ìš”ì•½í•˜ì„¸ìš”.
ì§ˆë¬¸ì— ë‹µí•˜ì§€ ë§ê³ , ë©”ëª¨ë¦¬ ì €ì¥ìš© ìš”ì•½ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

[Conversation]
{raw_text}""".strip()

    try:
        resp = _chat_hf.invoke(summary_prompt)
        summary_text = (
            resp.content if hasattr(resp, "content") else str(resp)
        ).strip()
    except Exception as e:
        _log(f"âš ï¸ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
        summary_text = raw_text[:1200]

    ts = datetime.now(timezone.utc).isoformat()
    mem_id = uuid.uuid4().hex

    raw_doc = Document(
        page_content=raw_text,
        metadata={
            "source": "chat_history_raw",
            "memory_id": mem_id,
            "saved_at": ts,
            "turn_count": 5,
            "message_count": MIN_MSGS,
        },
    )
    summary_doc = Document(
        page_content=summary_text,
        metadata={
            "source": "chat_history_summarized",
            "memory_id": mem_id,
            "saved_at": ts,
            "turn_count": 5,
            "message_count": MIN_MSGS,
        },
    )

    try:
        _raw_ingest_docs(
            documents=[raw_doc],
            persist_directory=PERSIST_DIR,
            collection_name=COLLECTION_CHAT_RAW,
            chunk_size=1200,
            chunk_overlap=120,
        )
        _raw_ingest_docs(
            documents=[summary_doc],
            persist_directory=PERSIST_DIR,
            collection_name=COLLECTION_CHAT_SUMMARY,
            chunk_size=400,
            chunk_overlap=40,
        )
        _log("âœ… save_memory ì™„ë£Œ (raw + summary ì €ì¥)")
    except Exception as e:
        _log(f"âš ï¸ save_memory ì €ì¥ ì‹¤íŒ¨: {e}")

    return {}


# ============================================================
# 12. ë¼ìš°íŒ… í•¨ìˆ˜ (conditional_edges ìš©)
# ============================================================


def retrieve_or_not(state: GraphState) -> str:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ë¬¸ì„œ ê²€ìƒ‰(retrieve)ì´ í•„ìš”í•œì§€ LLM ìœ¼ë¡œ íŒë‹¨.
    - ê²€ìƒ‰ ë¶ˆí•„ìš” â†’ "not retrieve" â†’ llm_answer ì§í–‰
    - ê²€ìƒ‰ í•„ìš”   â†’ "retrieve"     â†’ retrieve ë…¸ë“œ
    """
    question = state.get("question", "")
    if not question:
        return "not retrieve"

    prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ë ¤ë©´ **ë¬¸ì„œ/ë²¡í„°DB ê²€ìƒ‰(retrieve)**ì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ì„¸ìš”.

íŒë‹¨ ê¸°ì¤€:
- ì¸ì‚¬, ê°ì •, ë‹¨ìˆœ ëŒ€í™”("ì•ˆë…•", "ê³ ë§ˆì›Œ", "ë­í•´" ë“±), ì¡ë‹´ â†’ ê²€ìƒ‰ ë¶ˆí•„ìš”
- ë¬¸ì„œì— ìˆì„ ë²•í•œ ì „ë¬¸ ì§€ì‹ ì§ˆë¬¸ â†’ ê²€ìƒ‰ í•„ìš”
- ìµœì‹  ì •ë³´/ë‰´ìŠ¤ â†’ ê²€ìƒ‰ í•„ìš”

ì§ˆë¬¸: {question}

*ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSON ë§Œ ì¶œë ¥.
{{"need_retrieve": "yes"}} ë˜ëŠ” {{"need_retrieve": "no"}}""".strip()

    try:
        resp = _chat_hf.invoke(prompt)
        text = (resp.content or "").strip()
        match = re.search(r'\{[^{}]*"need_retrieve"[^{}]*\}', text)
        if match:
            data = json.loads(match.group(0))
            need = (data.get("need_retrieve") or "no").lower()
            if need in ("yes", "true", "1"):
                _log("ğŸ“– â†’ retrieve ë…¸ë“œë¡œ ì´ë™")
                return "retrieve"
        _log("ğŸ’¬ â†’ llm_answer ë…¸ë“œë¡œ ì§í–‰")
        return "not retrieve"
    except Exception:
        return "retrieve"  # ì—ëŸ¬ ì‹œ ì•ˆì „í•˜ê²Œ ê²€ìƒ‰ ì‹¤í–‰


def is_relevant(state: GraphState) -> str:
    """ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼ì— ë”°ë¼ ë¶„ê¸°"""
    return "relevant" if state.get("relevance") == "yes" else "not relevant"


def save_or_not(state: GraphState) -> str:
    """ë©”ì‹œì§€ ìˆ˜ > 20 ì´ë©´ save_memory ë¡œ ë¶„ê¸°"""
    return "save_chat" if len(state.get("messages", [])) > 20 else "too short"


# ============================================================
# 13. ê·¸ë˜í”„ êµ¬ì„± Â· ì»´íŒŒì¼
# ============================================================


def build_app():
    """
    LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•˜ê³  ì»´íŒŒì¼í•œë‹¤.

    ê·¸ë˜í”„ êµ¬ì¡°:
      START â†’ contextualize
              â”œâ”€ (retrieve í•„ìš”)  â†’ retrieve â†’ relevance_check
              â”‚                                â”œâ”€ (relevant)     â†’ llm_answer
              â”‚                                â””â”€ (not relevant) â†’ web_search â†’ llm_answer
              â””â”€ (retrieve ë¶ˆí•„ìš”) â†’ llm_answer
                                     â”œâ”€ (save_chat) â†’ save_memory â†’ END
                                     â””â”€ (too short) â†’ END

    Returns:
        ì»´íŒŒì¼ëœ LangGraph ì•±
    """
    global _app

    workflow = StateGraph(GraphState)

    # â”€â”€ ë…¸ë“œ ë“±ë¡ â”€â”€
    workflow.add_node("contextualize", contextualize)
    workflow.add_node("save_memory", save_memory)
    workflow.add_node("retrieve", retrieve)
    workflow.add_node("llm_answer", llm_answer)
    workflow.add_node("relevance_check", relevance_check)
    workflow.add_node("web_search", web_search)

    # â”€â”€ ì§„ì…ì  â”€â”€
    workflow.set_entry_point("contextualize")

    # â”€â”€ ì¡°ê±´ë¶€ ì—£ì§€: contextualize â†’ retrieve | llm_answer â”€â”€
    workflow.add_conditional_edges(
        "contextualize",
        retrieve_or_not,
        {"retrieve": "retrieve", "not retrieve": "llm_answer"},
    )

    # â”€â”€ retrieve â†’ relevance_check â”€â”€
    workflow.add_edge("retrieve", "relevance_check")

    # â”€â”€ ì¡°ê±´ë¶€ ì—£ì§€: relevance_check â†’ llm_answer | web_search â”€â”€
    workflow.add_conditional_edges(
        "relevance_check",
        is_relevant,
        {"relevant": "llm_answer", "not relevant": "web_search"},
    )

    # â”€â”€ web_search â†’ llm_answer â”€â”€
    workflow.add_edge("web_search", "llm_answer")

    # â”€â”€ ì¡°ê±´ë¶€ ì—£ì§€: llm_answer â†’ save_memory | END â”€â”€
    workflow.add_conditional_edges(
        "llm_answer",
        save_or_not,
        {"save_chat": "save_memory", "too short": END},
    )

    # â”€â”€ save_memory â†’ END â”€â”€
    workflow.add_edge("save_memory", END)

    # â”€â”€ ì»´íŒŒì¼ (MemorySaver: ì¸ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„°) â”€â”€
    memory = MemorySaver()
    _app = workflow.compile(checkpointer=memory)
    _log("âœ… LangGraph ì•± ì»´íŒŒì¼ ì™„ë£Œ")
    return _app


# ============================================================
# 14. ê³µê°œ API
# ============================================================


def query(question: str, thread_id: str | None = None) -> Dict[str, Any]:
    """
    ì§ˆë¬¸ì„ ì‹¤í–‰í•˜ê³  ìµœì¢… GraphState ë¥¼ ë°˜í™˜.

    Args:
        question:  ì‚¬ìš©ì ì§ˆë¬¸ ë¬¸ìì—´
        thread_id: ëŒ€í™” ì„¸ì…˜ ID (None ì´ë©´ ìë™ ìƒì„±)

    Returns:
        ìµœì¢… ìƒíƒœ ë”•ì…”ë„ˆë¦¬ (question, context, answer, messages, relevance)
    """
    if _app is None:
        raise RuntimeError("build_app() ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”")

    if thread_id is None:
        thread_id = random_uuid()

    config = RunnableConfig(
        recursion_limit=10,
        configurable={"thread_id": thread_id},
    )
    inputs = GraphState(question=question)

    # stream ëª¨ë“œë¡œ ì‹¤í–‰ â€” ê° ë…¸ë“œ ì™„ë£Œ ì‹œ ë¡œê·¸
    for event in _app.stream(inputs, config=config):
        for node_name in event:
            _log(f"ğŸ”„ {node_name} ë…¸ë“œ ì‹¤í–‰ ì™„ë£Œ")

    return _app.get_state(config).values


def get_app():
    """ì»´íŒŒì¼ëœ LangGraph ì•± ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return _app


def is_initialized() -> bool:
    """íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ ì—¬ë¶€"""
    return _initialized
