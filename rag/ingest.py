"""
ChromaDB ë¬¸ì„œ ì ì¬ ìœ í‹¸ë¦¬í‹°

ë‹¤ì–‘í•œ ì†ŒìŠ¤ì˜ ë¬¸ì„œë¥¼ ChromaDBì— ë²¡í„°í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
í”„ë¡œë•ì…˜ ì„œë¹„ìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì ì¬í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

Usage:
    # Pythonì—ì„œ ì§ì ‘ ì‚¬ìš©
    from rag.ingest import ingest_pdfs
    ingest_pdfs(["data/nlp.pdf"], persist_directory="./chroma_db")

    # CLIì—ì„œ ì‚¬ìš©
    python -m rag.ingest data/nlp.pdf data/transformer.pdf
"""

from langchain_community.document_loaders import PDFPlumberLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from typing import List, Optional, Union

# âš ï¸ ì„ë² ë”© ëª¨ë¸ì€ base.pyì—ì„œ ì¤‘ì•™ ê´€ë¦¬ (ì ì¬/ê²€ìƒ‰ ì‹œ ë™ì¼ ëª¨ë¸ ë³´ì¥)
from rag.base import create_embedding_auto as create_embedding


def ingest_pdfs(
    pdf_paths: List[str],
    persist_directory: str = "./chroma_db",
    collection_name: str = "default",
    chunk_size: int = 300,
    chunk_overlap: int = 50,
) -> Chroma:
    """
    PDF íŒŒì¼ë“¤ì„ ChromaDBì— ì ì¬í•©ë‹ˆë‹¤.

    Args:
        pdf_paths: PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        persist_directory: ChromaDB ì €ì¥ ë””ë ‰í† ë¦¬
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        chunk_size: í…ìŠ¤íŠ¸ ë¶„í•  í¬ê¸°
        chunk_overlap: í…ìŠ¤íŠ¸ ë¶„í•  ì˜¤ë²„ë©

    Returns:
        Chroma ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤
    """
    # 1. ë¬¸ì„œ ë¡œë“œ
    docs = []
    for path in pdf_paths:
        loader = PDFPlumberLoader(path)
        loaded = loader.load()
        docs.extend(loaded)
        print(f"  ğŸ“„ {path}: {len(loaded)}í˜ì´ì§€ ë¡œë“œ")

    print(f"ğŸ“„ ì´ {len(docs)}ê°œ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")

    # 2. í…ìŠ¤íŠ¸ ë¶„í• 
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
    )
    split_docs = splitter.split_documents(docs)
    print(f"âœ‚ï¸ ì´ {len(split_docs)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

    # 3. ChromaDBì— ì €ì¥
    embedding = create_embedding()
    vectorstore = Chroma.from_documents(
        documents=split_docs,
        embedding=embedding,
        persist_directory=persist_directory,
        collection_name=collection_name,
    )
    print(
        f"âœ… ChromaDB ì €ì¥ ì™„ë£Œ: {persist_directory} "
        f"(collection: {collection_name}, {len(split_docs)}ê°œ ì²­í¬)"
    )

    return vectorstore


def ingest_documents(
    file_paths: Optional[List[str]] = None,
    documents: Optional[List[Document]] = None,
    persist_directory: str = "./chroma_db",
    collection_name: str = "default",
    chunk_size: int = 300,
    chunk_overlap: int = 50,
) -> Chroma:
    """
    í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” LangChain Document ë¦¬ìŠ¤íŠ¸ë¥¼ ChromaDBì— ì ì¬í•©ë‹ˆë‹¤.

    Args:
        file_paths: í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (.txt, .md ë“±)
        documents: ì´ë¯¸ ë¡œë“œëœ LangChain Document ë¦¬ìŠ¤íŠ¸ (file_pathsì™€ íƒ 1)
        persist_directory: ChromaDB ì €ì¥ ë””ë ‰í† ë¦¬
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        chunk_size: í…ìŠ¤íŠ¸ ë¶„í•  í¬ê¸°
        chunk_overlap: í…ìŠ¤íŠ¸ ë¶„í•  ì˜¤ë²„ë©

    Returns:
        Chroma ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤

    Usage:
        # íŒŒì¼ ê²½ë¡œë¡œ ì ì¬
        ingest_documents(file_paths=["highmath12.txt", "notes.md"])

        # ì´ë¯¸ ë¡œë“œëœ Document ê°ì²´ë¡œ ì ì¬
        ingest_documents(documents=[Document(page_content="...", metadata={...})])
    """
    if file_paths is None and documents is None:
        raise ValueError("file_paths ë˜ëŠ” documents ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.")

    # íŒŒì¼ ê²½ë¡œê°€ ì£¼ì–´ì§„ ê²½ìš° â†’ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ
    docs = []
    if file_paths:
        for path in file_paths:
            try:
                loader = TextLoader(path, encoding="utf-8")
                loaded = loader.load()
                docs.extend(loaded)
                print(f"  ğŸ“„ {path}: {len(loaded)}ê°œ ë¬¸ì„œ ë¡œë“œ")
            except Exception as e:
                print(f"  âš ï¸ {path} ë¡œë“œ ì‹¤íŒ¨: {e}")
        print(f"ğŸ“„ ì´ {len(docs)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")

    # ì´ë¯¸ ë¡œë“œëœ Document ê°ì²´ê°€ ì£¼ì–´ì§„ ê²½ìš°
    if documents:
        docs.extend(documents)
        print(f"ğŸ“„ Document ê°ì²´ {len(documents)}ê°œ ì¶”ê°€")

    if not docs:
        print("âš ï¸ ì ì¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # í…ìŠ¤íŠ¸ ë¶„í• 
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
    )
    split_docs = splitter.split_documents(docs)
    print(f"âœ‚ï¸ ì´ {len(split_docs)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

    # ChromaDBì— ì €ì¥
    embedding = create_embedding()
    vectorstore = Chroma.from_documents(
        documents=split_docs,
        embedding=embedding,
        persist_directory=persist_directory,
        collection_name=collection_name,
    )
    print(
        f"âœ… ChromaDB ì €ì¥ ì™„ë£Œ: {persist_directory} "
        f"(collection: {collection_name}, {len(split_docs)}ê°œ ì²­í¬)"
    )

    return vectorstore


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python -m rag.ingest <pdf_path1> [pdf_path2] ...")
        print("Example: python -m rag.ingest data/nlp.pdf data/transformer.pdf")
        sys.exit(1)

    ingest_pdfs(sys.argv[1:])
