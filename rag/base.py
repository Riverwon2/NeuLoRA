from abc import ABC, abstractmethod
from operator import itemgetter
import os

# LangChain Core ë° Community
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import FAISS
from langchain_classic import hub

# Hugging Face ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ (OpenAI ëŒ€ì²´)
from langchain_huggingface import HuggingFaceEmbeddings, ChatHuggingFace, HuggingFaceEndpoint
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# â”€â”€â”€ ì„ë² ë”© ì„¤ì • (ingest.py, chroma.py ë“±ì—ì„œë„ ì´ ì„¤ì •ì„ ê³µìœ ) â”€â”€â”€â”€â”€â”€â”€â”€
# âš ï¸ ì ì¬(ingest)ì™€ ê²€ìƒ‰(retrieve) ì‹œ ë°˜ë“œì‹œ ë™ì¼í•œ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤!
EMBEDDING_MODEL = "BAAI/bge-m3"  # ì„ë² ë”© ëª¨ë¸ëª…
EMBEDDING_DEVICE = "auto"                         # "cpu" / "cuda" / "auto"


def create_embedding_local():
    """
    [ë¡œì»¬ ë°©ì‹] ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œì»¬ì—ì„œ ì‹¤í–‰
    - ìµœì´ˆ ì‹¤í–‰ ì‹œ ~/.cache/huggingface/hub/ ì— ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
    - ì´í›„ ìºì‹œì—ì„œ ë¡œë“œ (ì˜¤í”„ë¼ì¸ ê°€ëŠ¥)
    - GPU í™œìš© ê°€ëŠ¥ â†’ ë¹ ë¥¸ ì„ë² ë”©
    """
    return HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": EMBEDDING_DEVICE},
        encode_kwargs={"normalize_embeddings": True},
    )


def create_embedding_api():
    """
    [API ë°©ì‹] HuggingFace Inference APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì„ë² ë”© ìƒì„±
    - ëª¨ë¸ì„ ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•ŠìŒ
    - HF_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”
    - ë„¤íŠ¸ì›Œí¬ í•„ìš”, ë¬´ë£Œ í‹°ì–´ëŠ” ì†ë„ ì œí•œ ìˆìŒ
    """
    from langchain_huggingface import HuggingFaceEndpointEmbeddings

    api_token = os.environ.get("HF_API_KEY")
    if not api_token:
        raise ValueError("HF_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    return HuggingFaceEndpointEmbeddings(
        model=EMBEDDING_MODEL,
        huggingfacehub_api_token=api_token,
    )


# â”€â”€â”€ ì„ë² ë”© ë°©ì‹ ì„ íƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ë³€ìˆ˜ EMBEDDING_MODEë¡œ ì œì–´: "local" (ê¸°ë³¸) ë˜ëŠ” "api"
def create_embedding_auto():
    """í™˜ê²½ë³€ìˆ˜ EMBEDDING_MODEì— ë”°ë¼ ë¡œì»¬/API ë°©ì‹ ìë™ ì„ íƒ"""
    mode = os.environ.get("EMBEDDING_MODE", "local").lower()
    if mode == "api":
        print(f"ğŸŒ ì„ë² ë”©: API ë°©ì‹ ({EMBEDDING_MODEL})")
        return create_embedding_api()
    else:
        print(f"ğŸ’» ì„ë² ë”©: ë¡œì»¬ ë°©ì‹ ({EMBEDDING_MODEL}, device={EMBEDDING_DEVICE})")
        return create_embedding_local()


class RetrievalChain(ABC):
    def __init__(self):
        self.source_uri = None
        self.k = 5

    @abstractmethod
    def load_documents(self, source_uris):
        """loaderë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        pass

    @abstractmethod
    def create_text_splitter(self):
        """text splitterë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        pass

    def split_documents(self, docs, text_splitter):
        """text splitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¶„í• í•©ë‹ˆë‹¤."""
        return text_splitter.split_documents(docs)

    def create_embedding(self):
        """
        Embeddings ìƒì„± (ë¡œì»¬/API ìë™ ì„ íƒ)
        - í™˜ê²½ë³€ìˆ˜ EMBEDDING_MODE="api" â†’ HF Inference API í˜¸ì¶œ (ë‹¤ìš´ë¡œë“œ ì—†ìŒ)
        - í™˜ê²½ë³€ìˆ˜ EMBEDDING_MODE="local" ë˜ëŠ” ë¯¸ì„¤ì • â†’ ë¡œì»¬ ë‹¤ìš´ë¡œë“œ í›„ ì‹¤í–‰
        """
        return create_embedding_auto()

    def create_vectorstore(self, split_docs):
        """VectorStore ìƒì„± (FAISS)"""
        return FAISS.from_documents(
            documents=split_docs, embedding=self.create_embedding()
        )

    def create_retriever(self, vectorstore):
        """Retriever ìƒì„± (MMR ë°©ì‹ ë“±)"""
        dense_retriever = vectorstore.as_retriever(
            search_type="similarity", search_kwargs={"k": self.k}
        )
        return dense_retriever

    def create_model(self):
        """
        LLM ëª¨ë¸ ìƒì„±
        - HuggingFaceEndpoint ê¸°ë°˜ Qwen/Qwen2.5-14B-Instruct ì‚¬ìš©
        """
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ í† í° í™•ì¸
        api_token = os.environ.get("HF_API_KEY")
        if not api_token:
            raise ValueError("HF_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        llm = HuggingFaceEndpoint(
            repo_id="Qwen/Qwen2.5-14B-Instruct",
            #repo_id="HuggingFaceH4/zephyr-7b-beta",
            task="conversational",
            max_new_tokens=512,
            temperature=0.3,
            huggingfacehub_api_token=api_token
        )
        return ChatHuggingFace(llm=llm)

    
    def create_prompt(self):
        """Prompt Template ë¡œë“œ"""
        # ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (í•„ìš”ì‹œ ëª¨ë¸ì— ë§ì¶° ë³€ê²½ ê°€ëŠ¥)
        return hub.pull("teddynote/rag-prompt-chat-history")

    def create_prompt_new(self):
        system_template = (
            "You are an assistant for question-answering tasks. "
            "Use the following pieces of retrieved context to answer "
            "the question. If you don't know the answer, say that you "
            "don't know. Be kind and friendly when user says life question. "
            "And answer the question based on the policy.\n\n"
            "Policy: {policy}\n\n"
            "keep answer concise. And answer in Korean if user asks in Korean.\n\n"
            "{context}"
        )
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_template),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}"),
        ])
        return prompt

    @staticmethod
    def format_docs(docs):
        return "\n".join([doc.page_content for doc in docs])

    def create_chain(self):
        """RAG ì²´ì¸ êµ¬ì„±"""
        # 1. ë¬¸ì„œ ë¡œë“œ
        docs = self.load_documents(self.source_uri)
        
        # 2. í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = self.create_text_splitter()
        split_docs = self.split_documents(docs, text_splitter)
        
        # 3. ë²¡í„° ì €ì¥ì†Œ ë° ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
        self.vectorstore = self.create_vectorstore(split_docs)
        self.retriever = self.create_retriever(self.vectorstore)
        
        # 4. ëª¨ë¸ ë° í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        model = self.create_model()
        prompt = self.create_prompt_new()
        
        # 5. ì²´ì¸ ì—°ê²°
        self.chain = (
            {
                "question": itemgetter("question"),
                "context": itemgetter("context"),
                "chat_history": itemgetter("chat_history"),
                "policy": itemgetter("policy"),
            }
            | prompt
            | model
            | StrOutputParser()
        )
        return self